[build]
build_dir = "./_site"
templates_dir = "./templates"
static_dir = "./static"

[overview]
name = "Matt Baughman"
titles = ["PhD Candidate", "Computer Scientist"]
headshot = "images/headshot.jpg"
contacts = [
    { name = "email", link = "mailto:mbaughman123@gmail.com" },
    { name = "github", link = "https://github.com/mattebaughman" },
    { name = "linkedin", link = "https://www.linkedin.com/in/mattebaughman/" },
    { name = "scholar", link = "https://scholar.google.com/citations?user=oehomH0AAAAJ" },
]
analytics = ""
source = "https://github.com/mattebaughman/mattebaughman.github.io/"
text = """
Hi there!
I recently completed my Ph.D. in Computer Science with <a href="https://labs.globus.org" target="_blank">Globus Labs</a> at the <a href="https://cs.uchicago.edu/" target="_blank">University of Chicago</a>,
where I was co-advised by <a href="https://cs.uchicago.edu/people/ian-foster/" target="_blank">Ian Foster</a> and <a href="https://kylechard.com/" target="_blank">Kyle Chard</a>.
My research spans high-performance computing, distributed systems, and deep learning frameworks.
I completed my Bachelors in Computer Science at the <a href="https://cs.utexas.edu/" target="_blank">University of Texas at Austin</a> and previously worked at Apple, Google, and the Texas Advanced Computing Center.
"""

[research]
sections = [
    { name = "Distributed Systems", text = "We are designing new programming paradigms which decouple communication from application design to enable multiple data movement methods depending on <i>where</i> data are moved, <i>what</i> are moved, or <i>when</i> they are moved. We are using these paradigms to build scalable scientific workflows, federated function-as-a-service platforms, and multi-agent systems." },
    { name = "Scalable Deep Learning", text = "We are exploring new techniques for improving deep learning training time and scalability by (1) exploiting scalable algorithms for second-order information approximation; (2) developing methods for adapting to different computer hardware by tuning computation and communication to maximize training speed; (3) exploring compression techniques to reduce communication overheads; and (4) enabling complex, hierarchical federated learning across diverse ecosystems of hardware." },
    { name = "AI for Science", text = "We are (1) training large (billion+ parameter) transformer-based language models on broad scientific literature to automate knowledge extraction; (2) developing frameworks for coupling AI and simulations on exascale supercomputers; and (3) building innovative and large-scale solutions to scientific challenges in genome evolution, next-generation battery design, and carbon capture."},
]

[projects]
github = "https://github.com/mattebaughman/"
links = [
    { name = "", link = "", description = "" },
]

[publications]
bibtex = "publications/baughman.bib"
publications_dir = "./config/publications"

[presentations]
presentations_dir = "./config/presentations"

[theses]
theses_dir = "./config/theses"
